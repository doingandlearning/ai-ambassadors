# Use Case Development & Implementation (2 hours)

## **Session Overview**
**Aim**: Identify and prioritise AI opportunities within each function, ready to feed into the hackathon.

**Context**: 25 eBay ambassadors need to discover, evaluate, and prioritize AI use cases that can drive real business value and be implemented in the hackathon.

---

## **Learning Objectives**
By the end of this session, participants will:
- Apply use case discovery techniques for their departments
- Use ROI and prioritisation frameworks to evaluate AI opportunities
- Develop prototyping methods for fast iteration
- Create implementation and adoption plans for AI solutions

---

## **Session Structure**

### **Opening & Context Setting (15 minutes)**
Here’s a set of **speaker notes** for your *Opening & Context Setting* section — flexible enough to keep conversational but structured enough to land the key points.

---

* **Set tone**
  * Welcome everyone back, acknowledge momentum from earlier sessions.
  * Position this session as the *bridge* between AI skills (coaching, storytelling) and the hackathon.

* **Why use case development matters**
  * Strong use cases are the difference between “AI experiments” and “AI that sticks.”
  * Without them, teams risk either doing flashy but irrelevant demos, or tackling problems too big for the timeframe.
  * Use cases make AI adoption concrete, tied to business outcomes.

* **Connection to business impact**
  * Good use cases map directly to ROI: saving time, cutting costs, improving customer experience, or reducing risk.
  * This is what senior leadership cares about — and what will make ambassadors credible change agents.
  * Use cases also lower barriers to adoption: colleagues are more likely to engage when they see AI solving *their* problem.

* **eBay-specific examples (pick 2–3)**
  * *Customer Service*: AI-generated draft responses cut handling time while keeping personalisation.
  * *Marketing*: AI-assisted A/B test analysis sped up campaign decisions.
  * *Fraud*: Pattern-detection models flagged suspicious activity earlier, reducing manual workload.
  * Frame these as evidence: “When we focus on clear, bounded problems, the pay-off is real.”

---

### Department Assessment (10 mins)

* **Introduce reflection**

  * Say: *“Before we start generating new ideas, let’s ground ourselves. What’s already happening in your area, and where are the biggest gaps?”*
  * Invite them to think in **three lenses**:

    1. Current AI use cases — formal or informal
    2. Pain points / opportunities — where AI *might* help
    3. Resource constraints — time, skills, tech

* **Facilitation approaches** (choose depending on group mood):

  * **Individual jot + table share** (quiet reflection, then share at tables).
  * Or **sticky note storm**: everyone writes one idea per note, group them by department.
  * If remote: quick Miro/MURAL/whiteboard clusters.

* **Prompts to use**

  * Where do you see repetitive, error-prone, or slow tasks?
  * What’s the most frustrating process in your department?
  * Is there any unused data that could help decision-making?
  * What have people already been experimenting with?
  * What’s stopping faster adoption — skills, access, or mindset?

* **Wrap this segment**

  * Pull out 2–3 themes you hear across departments.
  * Note: *“We’ll revisit these pain points when we brainstorm use cases — they’re the raw material.”*

---

⚡ Tip: keep this section *grounded but brisk*. The risk is that people spend 15 minutes debating current pain points. Your role is to acknowledge but move them quickly into *solution-generating* mode.

---

### **Use Case Discovery Workshop (45 minutes)**

### Frame the Activity (5 mins)

* Start by linking back to the opening: *“We’ve surfaced pain points and opportunities — now we’re going to turn those into concrete AI use cases.”*
* Emphasise that **this is divergent thinking**: we want breadth, not polished solutions.
* Outline the four discovery techniques:
  * **Pain Point Analysis** → repetitive, time-consuming, or error-prone tasks.
  * **Data Opportunities** → unused or underutilised data sources.
  * **Process Mapping** → inefficiencies or bottlenecks.
  * **Stakeholder Needs** → what colleagues ask for most often.
* Give reassurance: *“Don’t worry yet about feasibility — that comes later in ROI. Right now, go wide.”*

---

### Department-Specific Prompts (5 mins)

* Offer tailored examples to spark thinking:
  * **Product** → feature prioritisation, analysing research faster.
  * **Finance** → fraud detection, automated reporting.
  * **Comms** → draft content, brand monitoring.
  * **Design** → variations, accessibility checks.
  * **Data Science** → speeding up model deployment or cleaning pipelines.
* Stress: these are *examples*, not a menu. Their own experience is more valuable.

---

### Group Brainstorming (20 mins)

* **Set-up**:
  * Give each group sticky notes or a shared digital space.
  * Ask for **at least 10 ideas per group** — quantity over quality.

* **Facilitation tips**:
  * Encourage one idea per sticky, so they can cluster or move them later.
  * If they get stuck, cycle back to the four techniques as prompts.
  * Circulate: ask probing questions like,
    * “Where do you spend the most manual time?”
    * “What’s something you *wish* you had data for?”
    * “If you could eliminate one bottleneck tomorrow, what would it be?”

---

### Sharing & Cross-Pollination (10 mins)
* Invite each department to post their notes on a wall/board.
* Ask them to walk around and see other groups’ lists.
* Prompt: *“Look for overlaps or opportunities where two departments could combine ideas into something bigger.”*
* Capture a couple of cross-functional sparks — e.g. Comms + Product on customer sentiment, or Finance + Data Science on anomaly detection.

---

### Wrap (5 mins)

* Reinforce that this was about **breadth, not filtering** — the prioritisation lens comes next.
* Summarise what you’ve seen: common themes (automation of repetitive work, insights from untapped data, etc).
* Transition: *“Now we’ve got raw material, the next step is to evaluate and prioritise which of these use cases deserve our time and investment.”*

---

⚡ *Facilitator tip:* Keep an eye on groups who start “solutionising” too quickly. Gently remind them: “Stay in problem/opportunity mode — we’ll test feasibility in the next step.”

---


- **Discovery Techniques**
  - **Pain Point Analysis**: What tasks are repetitive, time-consuming, or error-prone?
  - **Data Opportunity Assessment**: What data do we have that could be leveraged?
  - **Process Mapping**: Where are the bottlenecks and inefficiencies?
  - **Stakeholder Interviews**: What do colleagues need help with?

- **Department-Specific Discovery**
  - **Product**: User research analysis, A/B testing, feature prioritization
  - **Finance**: Fraud detection, risk assessment, financial reporting
  - **Comms**: Content creation, social media management, brand monitoring
  - **Design**: User research, design variations, accessibility testing
  - **Data Science**: Model deployment, data quality, insight generation

- **Interactive Exercise: "Use Case Brainstorming"**
  - Participants work in department groups
  - Generate 10+ potential AI use cases
  - Share and discuss with other departments
  - Identify cross-functional opportunities

### **ROI and Prioritisation Frameworks (30 minutes)**

### Framing the Shift (2–3 mins)

* Transition from discovery: *“We now have a wall full of ideas. The challenge is: which ones are worth our limited time and energy?”*
* Stress the need for **filters** — not all use cases are equal. Some are shiny but shallow, others are messy but high-impact.
* Introduce ROI + prioritisation frameworks as the tools for this filtering.

---

### ROI Calculation Framework (7–8 mins)

* Walk through the four levers with simple examples:

  * **Time Savings** → e.g., automating weekly reporting saves 4 hours per analyst per week.
  * **Cost Reduction** → fewer manual errors in finance saves rework, compliance fines.
  * **Revenue Impact** → improving product recommendations leads to more sales.
  * **Risk Mitigation** → fraud detection reduces financial loss and reputational damage.
* Optional humour: show **XKCD #1205 “Is It Worth the Time?”** cartoon.

  * Link it to ROI: *“Don’t spend a week automating something that only saves you 30 seconds once a month.”*
* Emphasise: even rough estimates help prioritise. We’re not doing a business case, just sizing value.

---

### Prioritisation Matrix (7–8 mins)

* Introduce the **Impact vs Effort 2×2 matrix**:

  * High impact / Low effort = *Quick wins*
  * High impact / High effort = *Strategic bets*
  * Low impact / Low effort = *Nice-to-haves*
  * Low impact / High effort = *Time sinks*
* Layer in other filters:

  * **Strategic alignment**: Does this support core business goals?
  * **Technical feasibility**: Do we have the skills/data/tools to build it?
  * **User readiness**: Will colleagues actually adopt this if built?
* Encourage participants to be honest: *“If you know your team won’t use it, even if it’s technically cool, maybe it’s not worth pursuing.”*

---

### Interactive Exercise – Use Case Evaluation (10–12 mins)

* Instructions:

  * Go back to your brainstormed use cases.
  * For each one, give it quick scores (1–5) on **impact** and **effort**.
  * Discuss alignment, feasibility, and user readiness as a group.
  * Use the matrix (sticky notes or digital board) to physically *place* each idea.
* Outcome:

  * Each department identifies their **top 3–5 use cases** to carry forward.
* Facilitation tip: walk around, listen for debates. If stuck, ask:

  * *“Which of these ideas, if we did nothing else, would make the biggest difference?”*
  * *“Which one is both doable and meaningful within a hackathon timeframe?”*

---

### Wrap and Transition (2–3 mins)

* Summarise: *“Now you’ve filtered your longlist into a shortlist of priority use cases. These are the seeds of your hackathon projects.”*
* Transition to prototyping: *“Next, we’ll look at how to quickly test and shape these ideas before you invest fully.”*

---

⚡ *Facilitator tip:* Keep energy high — ROI and scoring can feel heavy. The cartoon + 2×2 visual keeps it lively and practical.


- **ROI Calculation Framework**
  - **Time Savings**: Hours saved per week/month
  - **Cost Reduction**: Reduced manual work, fewer errors
  - **Revenue Impact**: Increased sales, better customer experience
  - **Risk Mitigation**: Reduced compliance issues, better security

- **Prioritisation Matrix**
  - **Impact vs. Effort**: High impact, low effort = quick wins
  - **Strategic Alignment**: How well does it support business goals?
  - **Technical Feasibility**: Can we actually build this?
  - **User Readiness**: Will people actually use this?

- **Interactive Exercise: "Use Case Evaluation"**
  - Participants evaluate their brainstormed use cases
  - Score each use case on impact, effort, alignment, and feasibility
  - Prioritize the top 3-5 use cases for their department

### **Prototyping Methods for Fast Iteration (30 minutes)**

### Frame the Mindset (3 mins)

* Transition: *“We’ve chosen our top use cases. Before building, we need to **prototype fast** — test ideas cheaply before committing big resources.”*
* Emphasise: prototyping reduces risk, reveals hidden challenges, and helps win buy-in.
* Message: *“Think ‘rough draft’, not ‘final product’. Good prototypes answer questions, not polish details.”*

---

### Rapid Prototyping Approaches (7–8 mins)

* **Paper Prototypes**

  * Quick sketches on paper or whiteboard of how a workflow or interface might look.
  * Useful for *early conversations*: “Would this flow make sense to you?”

* **Mockups**

  * Visuals created in Figma, PowerPoint, or Slides.
  * Show how data or AI outputs might appear in context.
  * Fast way to get feedback on layout and clarity.

* **MVP Development**

  * Minimum Viable Product: the smallest working piece that demonstrates value.
  * Example: a single Airtable automation or GPT prompt that saves time in one step.

* **A/B Testing**

  * Compare two lightweight versions to see which works better.
  * Example: two prompt versions → which generates clearer insights?

* Key message: *“Don’t over-invest. Each level is about proving the next step, not building a polished tool.”*

---

### AI-Specific Prototyping Tools (7–8 mins)

* **Custom GPTs**

  * Build a quick assistant around a single task (e.g., presentation coach, fraud-checking helper).
  * No code required; knowledge and context do the heavy lifting.
  * Live demo option: show “PresentationGPT” with style guides + actions.

* **Glean Integration**

  * For internal search and discovery; useful in prototyping knowledge access.

* Wrap point: *“Each tool lets you test a piece of your use case without needing a full system build.”*

---

### Interactive Exercise: Prototype Planning (10–12 mins)

* **Instructions**:

  * Choose 1–2 of your prioritised use cases.
  * Sketch a **prototype plan**:

    * What *question* do we want the prototype to answer?
    * What’s the **fastest way** to test it (paper, mockup, MVP, GPT)?
    * What does success look like? (time saved, clarity, ease of use)
    * What resources or tools would we need?
    * How long would this take? (days, not months)

* **Facilitation tip**:

  * Encourage ambitious but realistic thinking — *“What can you prove in a week, not a quarter?”*
  * Circulate, ask:

    * “What’s the smallest version of this idea?”
    * “What assumption are you testing first?”
    * “If this fails, what will you have learned?”

---

### Wrap (2 mins)

* Summarise: *“Prototypes help us test cheaply, fail safely, and learn quickly. This mindset is essential for the hackathon and beyond.”*
* Bridge: *“Now let’s think about how we take promising prototypes and build adoption plans that stick.”*

---

⚡ *Facilitator tip*: If energy is dipping, make this section more **hands-on sketching** (paper, sticky notes) rather than discussion-heavy.

---

- **Rapid Prototyping Approaches**
  - **Paper Prototypes**: Sketch out the user experience
  - **Mockups**: Create visual representations of the solution
  - **MVP Development**: Build the simplest version that works
  - **A/B Testing**: Compare different approaches

- **AI-Specific Prototyping Tools**
  - **Custom GPTs**: Quick chatbot and assistant prototypes
  - **Figma Integration**: Design prototypes with AI assistance
  - **Airtable Workflows**: Data processing and automation prototypes
  - **Glean Integration**: Search and discovery prototypes

- **Interactive Exercise: "Prototype Planning"**
  - Participants plan prototypes for their top use cases
  - Define success metrics and testing approaches
  - Identify resources and timeline needed

### **Implementation and Adoption Planning (30 minutes)**
### Framing the Section (2–3 mins)

* Transition: *“We’ve brainstormed use cases, prioritised them, and thought about prototypes. The last step is: how do we make sure these solutions actually stick?”*
* Emphasise: AI projects fail less often because of tech, more often because of adoption.
* Message: *“Our role as ambassadors isn’t just to build — it’s to guide teams through change.”*

---

### Implementation Roadmap (10–12 mins)

Walk through phases with relatable examples:

* **Phase 1 – Proof of Concept (PoC)**

  * Test on a tiny scale to validate assumptions.
  * Example: run a GPT prompt on 20 sample cases to see if it saves time.
  * *Key question*: “Does this even work in our context?”

* **Phase 2 – Pilot with Small Group**

  * Introduce the prototype to a handful of users.
  * Example: one finance sub-team using an AI reporting tool.
  * *Key question*: “Do users find this useful enough to keep going?”

* **Phase 3 – Department Rollout**

  * Broaden use with training and support.
  * Example: rolling the same tool across the entire finance department.
  * *Key question*: “How do we maintain quality and consistency?”

* **Phase 4 – Organisation Scale**

  * Integrate with systems, policies, and leadership priorities.
  * Example: embedding the tool into company-wide workflows.
  * *Key question*: “How do we ensure sustainability and alignment with strategy?”

---

### Adoption Strategies (10 mins)

* **Change Management**

  * Communicate clearly: why we’re doing this, what’s in it for them.
  * Avoid “AI hype” → focus on tangible benefits.

* **User Engagement**

  * Involve users early; let them shape the solution.
  * Share quick wins to build momentum.

* **Success Metrics**

  * Track adoption (number of users), efficiency (time saved), quality (error reduction), sentiment (surveys).
  * Keep metrics simple and relevant to the department.

* **Continuous Improvement**

  * Create a feedback loop → adjust based on real use.
  * Normalise iteration: “Version 1 is never perfect.”

---

### Interactive Exercise – “Adoption Roadmap” (10 mins)

* **Instructions:**

  * Take your top use case.
  * Sketch a simple roadmap across the 4 phases.
  * Identify:

    * Who are the first users (pilot group)?
    * What’s the first metric you’ll track?
    * What training/support will be needed?
    * What risks or blockers might appear?

* **Facilitation tip:**

  * Push for concreteness. If groups stay vague (“we’ll pilot in Q2”), ask:

    * “With who, exactly?”
    * “What would success in the pilot look like?”

---

### Wrap (2–3 mins)

* Reinforce: *“Implementation is where value is realised. Without adoption, even the best prototype is just a demo.”*
* Bridge to hackathon: *“In the hackathon, think not only about building, but how your project could live on afterwards.”*

---

⚡ *Facilitator tip:* Keep this section practical — avoid abstract talk about “change management theory.” Anchor everything in real eBay contexts and quick wins.


- **Implementation Roadmap**
  - **Phase 1**: Proof of concept and initial testing
  - **Phase 2**: Pilot with small group of users
  - **Phase 3**: Rollout to broader department
  - **Phase 4**: Scale across the organization

- **Adoption Strategies**
  - **Change Management**: Communication, training, support
  - **User Engagement**: Involve users in design and testing
  - **Success Metrics**: Define and track key performance indicators
  - **Continuous Improvement**: Regular feedback and iteration

- **Interactive Exercise: "Implementation Planning"**
  - Participants create implementation plans for their use cases
  - Identify stakeholders, resources, and timeline
  - Plan for change management and user adoption

### **Wrap-up & Action Planning (15 minutes)**
- **Key Takeaways Review**
- **Personal Use Case Goals** for each ambassador
- **Hackathon Project Selection** and team formation
- **Next Steps** and implementation planning

---
x